squad! anyone interested in sharing a room in Philly? <@U16RAECF5>: Just got your voicemail, I was in a faculty meeting. Holler at me if you’re otherwise undisposed. This study is that _straight_ dope. But I want a <@U16TY5M6F> viewpoint on their model. In particular, they’re interpreting their item difficulty parameters in a _really_ literal sense (literally the probability that a traffic stop will occur). I’d love to read closely and talk with anyone who has also read it closely, though I know that you are all probably at APSA right now. <https://5harad.com/papers/threshold-test.pdf> Here’s the git repo to their work. <https://github.com/5harad/threshold-test> holy crap. this shit is good. I'd also be really interested to hear Chris's take In particular, the core of their result hinges on a literal interpretation of the item-difficulty parameters that they estimate. I’m not deep enough into this type of model to know if that is warrented. *warranted. I’m bringing it to the central list... because those fools in <#C16S81202|bureaucrats> weren’t paying attention! I’ve got a three way interaction that I’ve got to interpret -- two conditioning factors and a treatment -- and there is no way that that journal I’m sending this to (California Journal of Politics and Policy) is going to let me get away with that. So, I’ve got to simplify the presentation somehow. Here was my thought: just present treatment effects (that is the first-difference on the treatment factor), in a `2x3` table where the dimensions on the table are the conditioning factors (one which has two levels and one which has three levels). The models that go into these estimates are the following: ``` m1 = glm(yvar ~ treatment, family = “binomial”) # this is going to be in black m2 = glm(yvar ~ treatment * structural, family = “binomial”) # in grey column m3 = glm(yvar ~ treatment * realized, family = “binomial”) # in grey row m4 = glm(yvar ~ treatment * realized * structural, family = “binomial”) # internal cells ``` <@U16RY7PR6> <@U16RLTH3N> <@U16S9N0LE> - so do we know of anyone who has data on the race of state leg.s? If not, how about the idea of using the facial recognition software tools we were looking at earlier to pull legislator race from publicly available pictures (FB profiles, pics on state web pages, etc.)? <@U16RAECF5> We could perhaps do that. Why state leg.s? I wonder if Adam (<http://adamdynes.com/>) might have info on municipal officials from his survey. <@U16RY7PR6> unless I'm mistaken there is a very low cost follow-on paper to our state legislator experiment in which we determine whether the race of state legislators impacts response rates. <@U16RAECF5> Gotcha. I would agree with that. I wonder what the state of the art is with name recognition software - since we already have the names of all the representatives. Or maybe we just MTurk the whole thing. We might be able to get urls for individual legislators from Sunlight Foundation. We could then ask MTurkers to load the pages and identify the representative’s race. Have each MTurker do each legislator at least once. Some back of the envelope calculations suggest that it would take around 250 hours to identify each one once this way. At 8 an hour, that is 2,000 plus Amazon fees. Of course, that is only for one coder. ok. i'm going to look into bringing someone on who might take care of these tasks. will get back to the group when I hear. The name software is pretty good these days, but we can know (by querying @nk in the the other channel) where it is specifically. He was on the team at IBM that wrote some of the name \rightarrow ethnicity software that IBM uses. Anyone have 2 minutes to talk about blocking and matching as a way to get at a heterogeneous treatment effect. I’m spinning my wheels now. OK - Jason S., new in Pol. Sci. at UGA, formerly at Berkeley, informs me that his facial recognition tool is ready to go, and could turn profile or other photos into an estimate of the race of state leg.s, and also a distance metric relative to a racial category. I think this would be a very straightforward, solid publication. I'll be meeting with him in a couple of weeks to talk through the specifics. In the meantime, if folks on the team could think about how we might get the photos together, that would be helpful. Here is something for the Congress, which we probably already knew existed? <https://www.gpo.gov/fdsys/browse/collection.action?collectionCode=GPO&amp;browsePath=Congressional+Pictorial+Directory> I’m asking my research librarian if he knows of other such resources. One possibility is the Congressional YellowBook. <@U16RAECF5>: You want to change our legal status to domestic partner so we can both apply at UT? That way one of us can get the spouse hire for the other? I do. Ok. But I cook better than I wash dishes, just so we know what the trade-off is. IIA. Btw, I think we do know about the photobook bc. some hngers were doing a project. The win here would be to get race for state leg.s, bc. there is a really valuable observational part. We can wrap that into the secondary analysis of the experiment, and together with the data and the tool, we have a fucking phatty pub. I’ll let you know what is out there when my librarian gets back with me. If Leadership Directories has put it together for us already, it would be _the_ dope. (BTW: I don’t know what IIA is...) Also, ask and you shall receive. I bring forth the data, or at least a demonstration that we can has the data. The question I continue to have is, what we’re pointing this variable at. We can’t assess electoral consequences of this unless we undertake the _bonkers_ task of finding a standardized picture of every candidate for office that lost. That shit jus aint gonna happen. So, we’re left with a bunch of unobserved selection into office, then a classifier that tells us something that we might already know (since we have that data series from these Quorum folks), and ... OK. first response, there is a kid at Berkely who has put something together: <http://www.christiandphillips.com/research-1/> From his website - "...new data containing district and candidate demographic information for every state legislative general election from 1996-2015". So we would be generating a competing data set, using a different approach. <@U16RLTH3N>, the answer to your question is very simple. This is an HTE that people care about. Period. That's what gets published in pol sci journals. If we don't do this follow up, somebody else will. Do we know how Christian sourced his data? Is there benefit to be had in a dataset (just as a dataset) that is produced in a different fashion, that covers 10% of an existing data set? To me, the data _qua_ data contribution here feels a little slender, especially if someone else already has it. _Adding_ the skin-tone based racial assessment makes it a bit more sexy, but I don’t think it overcomes the hurdle that this fellow has the drop on us. Having the data for our own purposes, to test the HTE, to me feels like a good justification. And I think you’re right that we should look at this either in this paper, or in a followup paper to this experiment. Does anybody have an in either at Pew or the NCSL? I wonder because they’ve published a _Stateline_ series, which at least has the ethnicity data that MR_BROWN and MR_ORANGE were thinking about. I’ve been warned that if I scrape the data from the group that I had posted about last week, that the group would shut down campus access and I would be persona non grata. <@U16RY7PR6> and <@U16TY5M6F> I’ll be in A2 Monday October 3. Let me know if you’re available and we can work on some of this boondoggle in person again. <@U16RLTH3N> I’m available all day except from 12-3. It’d be great to get together. Woot! Maybe in the morning? I haven’t been on campus in Ann Arbor in a while... Totally! Do you have a place to stay? <!channel>: One of my co-authors is building a topic classifier that is functioning on Slack data. I want to export this and give it to the student/co-author. Would this make you sad? Would this violate your rights? Would this give away trade secrets? 